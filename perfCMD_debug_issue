cp config93GB.json config.json
numactl -l -N 0 ./skyriver  -debugDirectory=n1022/logs_0


/home/regression/tools/mutilateGA
./mutilate -s 10.0.0.22 -d 1 -c 128 -T 12 -t 600 -r 10000 -V 1000000 -K 13 -u 1.000000 --noload --affinity

iostat or bmon


/proc/sys/net/ipv4$ cat tcp_rmem 
4096	87380	6291456

/proc/sys/net/core$ cat rmem_default 
212992
regression@n1022:/proc/sys/net/core$ cat rmem_max 
212992
regression@n1022:/proc/sys/net/core$ cat wmem_default 
212992
regression@n1022:/proc/sys/net/core$ cat wmem_max 
212992
regression@n1022:/proc/sys/net/core$ cat optmem_max 
20480
regression@n1022:/proc/sys/net/core$ cat netdev_max_backlog 
1000

sysctl -a | grep mem
net.core.optmem_max = 20480
net.core.rmem_default = 212992
net.core.rmem_max = 212992
net.core.wmem_default = 212992
net.core.wmem_max = 212992

net.ipv4.tcp_mem = 6189357	8252476	12378714
net.ipv4.tcp_rmem = 4096	87380	6291456
net.ipv4.tcp_wmem = 4096	16384	4194304
net.ipv4.udp_mem = 12378714	16504952	24757428
net.ipv4.udp_rmem_min = 4096
net.ipv4.udp_wmem_min = 4096

net.ipv4.tcp_no_metrics_save = 0

$ sudo sysctl -w net.ipv4.tcp_rmem='4096 87380 32765'


# test env:
server used: 33 (skyriver), 32(mutilate), 26(mutilate)

# Test commands:

# skyriver
~/Skyriver_Binary/Kessel1.0rc3$ numactl -l -N 0 ./skyriver  -debugDirectory=n1033/logs_0

# iperf server
iperf -s

# iperf client
iperf -c 10.0.0.22 -i 5 -t 10 -l 1M

# perf tool watchers on n1033
iostat 2
bmon

# 1 or 2 mutilate instances:
numactl --localalloc --cpunodebind=0 ./mutilate -s 10.0.0.33 -d 1 -c 128 -T 12 -t 600 -r 10000 -V 1000000 -K 13 -u 1.000000 --noload --affinity
numactl --localalloc --cpunodebind=1 ./mutilate -s 10.0.0.33 -d 1 -c 128 -T 12 -t 600 -r 10000 -V 1000000 -K 13 -u 1.000000 --noload --affinity

# result1: 1 instance
nvme1n1        8657.50    513328.00    533248.00    1026656    1066496
nvme0n1        8791.00    396462.00    665216.00     792924    1330432
  p27p2                        │   1.15GiB 813.63K     │   4.76MiB  75.36K

# result2: 2 instances from 2 servers
nvme1n1        2183.50    131924.00    131072.00     263848     262144
nvme0n1        2122.50     31564.00    223360.00      63128     446720
  p27p2                        │ 390.12MiB 270.40K     │   6.00MiB  82.21K

# result3: 2 instances from 1 server
numactl --localalloc --cpunodebind=1
nvme1n1        9997.50    604292.00    620544.00    1208584    1241088
nvme0n1        9559.00    664528.00    497984.00    1329056     995968
  p27p2                        │   1.15GiB 813.14K     │   6.55MiB 103.91K

# after 10-20 seconds, numbers dropped and varies
nvme1n1        5300.50    346834.00    307584.00     693668     615168
nvme0n1        4859.50    348594.00    250304.00     697188     500608
  p27p2                        │ 560.96MiB 388.79K     │   7.08MiB  99.95K
nvme1n1        3217.50    132024.00    262144.00     264048     524288
nvme0n1        2686.50    262988.00     64320.00     525976     128640
  p27p2                        │ 479.50MiB 332.34K     │   5.82MiB  82.69K

# result4: 1 instance and 1 iperf
nvme1n1        3049.00    244454.00    131072.00     488908     262144
nvme0n1        2185.50    131958.00    131072.00     263916     262144
  p27p2                        │   1.05GiB 746.16K     │   6.59MiB  90.38K

# result5: 1 instance, but double the connections & threads
./mutilate -s 10.0.0.33 -d 1 -c 256 -T 24 -t 600 -r 10000 -V 1000000 -K 13 -u 1.000000 --noload --affinity
nvme1n1        4601.00    320812.00    242176.00     641624     484352
nvme0n1        4832.50    263594.00    327360.00     527188     654720
  p27p2                        │ 530.85MiB 367.94K     │   6.82MiB  95.81K

# result6: 1 instance, double the threads
nvme1n1        4314.50    263480.00    262144.00     526960     524288
nvme0n1        5047.50    263568.00    355008.00     527136     710016
  p27p2                        │ 503.60MiB 349.06K     │   5.82MiB  81.62K

# result7: new mutilate cmd to reduce connections
numactl --localalloc --cpunodebind=0 ./mutilate -s 10.0.0.33 -d 1 -c 8 -T 12 -t 600 -r 10000 -V 1000000 -K 13 -u 1.000000 --noload --affinity
numactl --localalloc --cpunodebind=1 ./mutilate -s 10.0.0.33 -d 1 -c 8 -T 12 -t 600 -r 10000 -V 1000000 -K 13 -u 1.000000 --noload --affinity

# result7: 1 instance
nvme1n1        8511.50    457376.00    569664.00     914752    1139328
nvme0n1        8714.50    527462.00    526784.00    1054924    1053568
  p27p2                        │   1.15GiB 814.25K     │  10.32MiB 163.59K

# result8: 2 instances from 1 server
nvme1n1        8974.00    437334.00    649344.00     874668    1298688
nvme0n1       10126.50    653058.00    581312.00    1306116    1162624
 p27p2                        │   1.15GiB 814.26K     │  12.92MiB 205.12K

# result9: 2 instances from 2 servers
nvme1n1        2737.00     68044.00    262144.00     136088     524288
nvme0n1        3237.00    263178.00    131072.00     526356     262144
  p27p2                        │ 363.20MiB 251.92K     │   4.27MiB  58.34K

# result10: 1 instance, 1 iperf from 1 server
nvme1n1        9414.00    625370.00    524288.00    1250740    1048576
nvme0n1        9086.50    528750.00    575104.00    1057500    1150208
  p27p2                        │   1.15GiB 814.13K     │  11.14MiB 176.47K

# result11: 1 instance, i iperf from 2 servers
nvme1n1        6948.50    526428.00    321024.00    1052856     642048
nvme0n1        5955.00    264298.00    456576.00     528596     913152
  p27p2                        │   1.14GiB 812.62K     │   8.66MiB 120.28K

# new setting: change net.ipv4.tcp_rmem='4096 8192 16384' instead of '4096 87380 6291456'
# result12: 1 instance, 2 iperf from 2 servers
nvme1n1        9566.50    658454.00    505856.00    1316908    1011712
nvme0n1        9213.00    603478.00    514432.00    1206956    1028864
  p27p2                        │   1.11GiB 839.46K     │  12.30MiB 186.97K

net.ipv4.tcp_mem = 6188841	8251789	12377682
net.ipv4.tcp_rmem = 4096	87380	6291456
net.ipv4.tcp_wmem = 4096	16384	4194304
net.ipv4.udp_mem = 12377682	16503579	24755364
net.ipv4.udp_rmem_min = 4096
net.ipv4.udp_wmem_min = 4096
net.core.optmem_max = 20480
net.core.rmem_default = 212992
net.core.rmem_max = 212992
net.core.wmem_default = 212992
net.core.wmem_max = 212992
net.ipv4.igmp_max_memberships = 20

net.ipv4.tcp_allowed_congestion_control = cubic reno
net.ipv4.tcp_available_congestion_control = cubic reno
net.ipv4.tcp_congestion_control = cubic
net.ipv4.tcp_mtu_probing = 0
net.core.default_qdisc = pfifo_fast


sudo sysctl -w net.core.rmem_max=67108864
sudo sysctl -w net.core.wmem_max=67108864
sudo sysctl -w 
sudo sysctl -w
sudo sysctl -w net.ipv4.tcp_congestion_control=htcp
sudo sysctl -w net.ipv4.tcp_mtu_probing=1
sudo sysctl -w net.core.default_qdisc=fq
sudo sysctl -w

# allow testing with buffers up to 64MB 
net.core.rmem_max = 67108864 
net.core.wmem_max = 67108864 
# increase Linux autotuning TCP buffer limit to 32MB
net.ipv4.tcp_rmem = 4096 87380 33554432
net.ipv4.tcp_wmem = 4096 65536 33554432
# recommended default congestion control is htcp 
net.ipv4.tcp_congestion_control=htcp
# recommended for hosts with jumbo frames enabled
net.ipv4.tcp_mtu_probing=1
# recommended for CentOS7/Debian8 hosts
net.core.default_qdisc = fq


# tools installed
sudo apt-get install iperf
sudo apt-get install linux-tools-common
sudo apt-get install linux-tools-4.4.0-53-generic
sudo apt-get install gdb

# perf top  
# 16k 2 instances 2 servers
   6.91%  skyriver                   [.] runtime.runqgrab
   4.13%  skyriver                   [.] runtime.findrunnable
   4.01%  [kernel]                   [k] copy_user_enhanced_fast_string
   2.85%  [unknown]                  [k] 0x00000000004b3f20
   1.99%  skyriver                   [.] runtime/internal/atomic.Load
   1.82%  skyriver                   [.] runtime.runqsteal
   1.78%  [kernel]                   [k] _raw_spin_lock
   1.66%  [kernel]                   [k] ixgbe_clean_rx_irq
   1.64%  [kernel]                   [k] native_write_msr_safe
   1.63%  [kernel]                   [k] _raw_spin_lock_irqsave
   1.36%  [kernel]                   [k] update_cfs_shares
   1.30%  [kernel]                   [k] __get_page_tail
   1.27%  [kernel]                   [k] __nf_conntrack_find_get
   1.27%  skyriver                   [.] runtime/internal/atomic.Cas
   1.25%  skyriver                   [.] runtime.runqempty
   1.19%  skyriver                   [.] runtime/internal/atomic.Xchg
   1.11%  [kernel]                   [k] copy_page_to_iter_iovec
   1.00%  [kernel]                   [k] native_queued_spin_lock_slowpath
   0.93%  [kernel]                   [k] menu_select
   0.90%  [kernel]                   [k] sock_rfree
   0.90%  [kernel]                   [k] effective_load.isra.43
   0.90%  [kernel]                   [k] __fget
   0.87%  [kernel]                   [k] __schedule
   0.84%  [kernel]                   [k] put_compound_page
   0.82%  [kernel]                   [k] update_curr
   0.73%  [kernel]                   [k] native_irq_return_iret
   0.73%  [kernel]                   [k] cpuidle_enter_state
   0.67%  [kernel]                   [k] set_next_entity
   0.66%  [kernel]                   [k] int_sqrt
   0.63%  [kernel]                   [k] gup_pte_range
   0.61%  [kernel]                   [k] dequeue_entity
   0.61%  [kernel]                   [k] compound_unlock_irqrestore
   0.61%  skyriver                   [.] runtime.memmove

# 6M 2 instances 2 servers
   7.56%  [kernel]                   [k] copy_user_enhanced_fast_string
   4.24%  [unknown]                  [k] 0x00000000004b3f20
   2.94%  [kernel]                   [k] ixgbe_clean_rx_irq
   2.87%  skyriver                   [.] runtime.runqgrab
   2.50%  [kernel]                   [k] __get_page_tail
   1.60%  [kernel]                   [k] dev_gro_receive
   1.59%  [kernel]                   [k] _raw_spin_lock
   1.59%  skyriver                   [.] runtime.findrunnable
   1.58%  [kernel]                   [k] put_compound_page
   1.47%  [kernel]                   [k] copy_page_to_iter_iovec
   1.26%  skyriver                   [.] runtime.memmove
   1.23%  [kernel]                   [k] native_write_msr_safe
   1.20%  [kernel]                   [k] put_page
   1.16%  [kernel]                   [k] put_page_testzero
   1.13%  [kernel]                   [k] compound_unlock_irqrestore
   1.12%  [kernel]                   [k] cpuidle_enter_state
   1.11%  [kernel]                   [k] _raw_spin_lock_irqsave
   0.97%  [kernel]                   [k] menu_select
   0.88%  [kernel]                   [k] native_irq_return_iret
   0.88%  [kernel]                   [k] __nf_conntrack_find_get
   0.87%  skyriver                   [.] runtime/internal/atomic.Load
   0.79%  [kernel]                   [k] update_cfs_shares
   0.72%  skyriver                   [.] runtime.runqsteal
   0.72%  [kernel]                   [k] ixgbe_poll
   0.71%  [kernel]                   [k] int_sqrt
   0.68%  [kernel]                   [k] nf_iterate
   0.66%  [kernel]                   [k] native_queued_spin_lock_slowpath
   0.65%  [kernel]                   [k] __free_page_frag
   0.63%  [kernel]                   [k] __build_skb
   0.59%  [unknown]                  [k] 0x00000000004b3f2e
   0.58%  [kernel]                   [k] inet_gro_receive
   0.55%  [kernel]                   [k] __schedule
   0.55%  [kernel]                   [k] eth_get_headlen
   0.53%  [kernel]                   [k] get_page_from_freelist
   0.52%  [kernel]                   [k] tcp_gro_receive
   0.50%  skyriver                   [.] runtime.runqempty
   0.50%  [kernel]                   [k] skb_copy_datagram_iter
   0.50%  [kernel]                   [k] memcpy_erms

# 6M 2 instances 1 server
   6.55%  skyriver                   [.] runtime.runqgrab
   5.69%  [kernel]                   [k] copy_user_enhanced_fast_string
   3.86%  skyriver                   [.] runtime.findrunnable
   2.82%  [unknown]                  [k] 0x00000000004b3f20
   1.86%  skyriver                   [.] runtime/internal/atomic.Load
   1.80%  [kernel]                   [k] native_write_msr_safe
   1.78%  [kernel]                   [k] ixgbe_clean_rx_irq
   1.66%  skyriver                   [.] runtime.runqsteal
   1.64%  [kernel]                   [k] __get_page_tail
   1.52%  [kernel]                   [k] _raw_spin_lock
   1.46%  [kernel]                   [k] copy_page_to_iter_iovec
   1.42%  [kernel]                   [k] _raw_spin_lock_irqsave
   1.37%  [kernel]                   [k] update_cfs_shares
   1.25%  skyriver                   [.] runtime/internal/atomic.Cas
   1.16%  skyriver                   [.] runtime/internal/atomic.Xchg
   1.15%  skyriver                   [.] runtime.runqempty
   1.10%  skyriver                   [.] runtime.memmove

# ps auwxf | grep skyriver
regress+ 35335  102  1.9 17544400 10448176 pts/0 Sl+ 10:38 288:15  |           \_ ./skyriver -debugDirectory=n1033/logs_0
regress+ 44857  0.0  0.0  11748  2144 pts/7    S+   15:19   0:00              \_ grep --color=auto skyriver

# ps auwxf | grep skyriver | awk '{print $2}'
35335
44861

$ sudo perf top -vzp 35335
  12.84%  /proc/kcore                                             0x7fff813e7685     k [k] copy_user_enhanced_fast_string
   7.08%  [unknown]                                               0x4b3f20           ! [k] 0x00000000004b3f20
   5.16%  /home/regression/Skyriver_Binary/Kessel1.0rc3/skyriver  0x8e616            d [.] runtime.runqgrab
   4.43%  /proc/kcore                                             0x7fff811937e2     k [k] __get_page_tail
   2.80%  /home/regression/Skyriver_Binary/Kessel1.0rc3/skyriver  0x87ee7            d [.] runtime.findrunnable
   2.71%  /proc/kcore                                             0x7fff813eeb72     k [k] copy_page_to_iter_iovec
   2.71%  /proc/kcore                                             0x7fff81192e8e     k [k] put_compound_page

/tmp$ gdb skyriver 

(gdb) info symbol 0x4b3f20
runtime.memmove + 160 in section .text

(gdb) disassemble 0x4b3f00
Dump of assembler code for function runtime.memmove:
   0x00000000004b3e80 <+0>:	mov    0x8(%rsp),%rdi
   0x00000000004b3e85 <+5>:	mov    0x10(%rsp),%rsi
   0x00000000004b3e8a <+10>:	mov    0x18(%rsp),%rbx
   0x00000000004b3e8f <+15>:	test   %rbx,%rbx
   0x00000000004b3e92 <+18>:	je     0x4b3f3a <runtime.memmove+186>
   0x00000000004b3e98 <+24>:	cmp    $0x2,%rbx
   0x00000000004b3e9c <+28>:	jbe    0x4b408e <runtime.memmove+526>
   0x00000000004b3ea2 <+34>:	cmp    $0x4,%rbx
   0x00000000004b3ea6 <+38>:	jbe    0x4b4216 <runtime.memmove+918>
   0x00000000004b3eac <+44>:	cmp    $0x8,%rbx
   0x00000000004b3eb0 <+48>:	jb     0x4b4209 <runtime.memmove+905>
   0x00000000004b3eb6 <+54>:	je     0x4b4202 <runtime.memmove+898>
   0x00000000004b3ebc <+60>:	cmp    $0x10,%rbx
   0x00000000004b3ec0 <+64>:	jbe    0x4b41f1 <runtime.memmove+881>
   0x00000000004b3ec6 <+70>:	cmp    $0x20,%rbx
   0x00000000004b3eca <+74>:	jbe    0x4b41dc <runtime.memmove+860>
   0x00000000004b3ed0 <+80>:	cmp    $0x40,%rbx
   0x00000000004b3ed4 <+84>:	jbe    0x4b41b1 <runtime.memmove+817>
   0x00000000004b3eda <+90>:	cmp    $0x80,%rbx
   0x00000000004b3ee1 <+97>:	jbe    0x4b415a <runtime.memmove+730>
   0x00000000004b3ee7 <+103>:	cmp    $0x100,%rbx
   0x00000000004b3eee <+110>:	jbe    0x4b409b <runtime.memmove+539>
   0x00000000004b3ef4 <+116>:	cmp    %rdi,%rsi
   0x00000000004b3ef7 <+119>:	jbe    0x4b4040 <runtime.memmove+448>
   0x00000000004b3efd <+125>:	cmp    $0x800,%rbx
   0x00000000004b3f04 <+132>:	jbe    0x4b3f3b <runtime.memmove+187>
   0x00000000004b3f06 <+134>:	testl  $0x200,0x84f0b0(%rip)        # 0xd02fc0 <runtime.cpuid_ebx7>
   0x00000000004b3f10 <+144>:	je     0x4b3f23 <runtime.memmove+163>
   0x00000000004b3f12 <+146>:	mov    %esi,%eax
   0x00000000004b3f14 <+148>:	or     %edi,%eax
   0x00000000004b3f16 <+150>:	test   $0x7,%eax
   0x00000000004b3f1b <+155>:	je     0x4b3f23 <runtime.memmove+163>
   0x00000000004b3f1d <+157>:	mov    %rbx,%rcx
=> 0x00000000004b3f20 <+160>:	rep movsb %ds:(%rsi),%es:(%rdi)
   0x00000000004b3f22 <+162>:	retq   
   0x00000000004b3f23 <+163>:	mov    %rbx,%rcx
   0x00000000004b3f26 <+166>:	shr    $0x3,%rcx
   0x00000000004b3f2a <+170>:	and    $0x7,%rbx
---Type <return> to continue, or q <return> to quit---
=> 0x00000000004b3f2e <+174>:	rep movsq %ds:(%rsi),%es:(%rdi)

$ cd /tmp
$ sudo perf record -F 99 -p 13583 -g -- sleep 5
$ sudo perf report -n

Samples: 6K of event 'cycles:pp', Event count (approx.): 318976310692                                           
  Children      Self       Samples  Command   Shared Object      Symbol                                         
+   71.01%     0.00%             0  skyriver  skyriver           [.] runtime.goexit
+   56.11%     0.00%             0  skyriver  skyriver           [.] main.(*ConnectionHandler).receiveRequestASC
+   56.11%     0.00%             0  skyriver  skyriver           [.] main.(*ConnectionHandler).(main.receiveRequ
+   56.11%     0.00%             0  skyriver  skyriver           [.] main.(*ConnectionHandler).serve
+   55.00%     0.00%             0  skyriver  skyriver           [.] main.(*ConnectionHandler).receiveArgumentsA
+   54.97%     0.00%             0  skyriver  skyriver           [.] main.(*ConnectionHandler).receiveStorageFmt
+   54.79%     0.85%            13  skyriver  skyriver           [.] github.com/OmniTier/skyriver/slab/readBuffe
+   53.85%     0.34%             6  skyriver  skyriver           [.] net.(*netFD).Read
+   53.50%     0.01%             1  skyriver  skyriver           [.] net.(*conn).Read
+   51.30%     0.75%            20  skyriver  [kernel.kallsyms]  [k] entry_SYSCALL_64_fastpath
+   48.14%     0.07%             4  skyriver  skyriver           [.] runtime.mcall
+   47.61%     0.19%             1  skyriver  skyriver           [.] runtime.park_m
+   47.49%     0.00%             0  skyriver  skyriver           [.] net.(*pollDesc).waitRead
+   47.15%     0.00%             0  skyriver  skyriver           [.] net.(*pollDesc).wait
+   47.14%     0.00%             0  skyriver  skyriver           [.] net.runtime_pollWait
+   47.08%     0.00%             1  skyriver  skyriver           [.] runtime.schedule
+   46.18%     0.14%             2  skyriver  skyriver           [.] runtime.netpollblock
+   40.31%     4.45%            91  skyriver  skyriver           [.] runtime.findrunnable
+   21.66%     0.33%             7  skyriver  [kernel.kallsyms]  [k] vfs_read
+   21.04%     0.00%             1  skyriver  [kernel.kallsyms]  [k] __vfs_read
+   21.04%     0.10%             3  skyriver  [kernel.kallsyms]  [k] new_sync_read
+   20.72%     0.11%             4  skyriver  skyriver           [.] syscall.Syscall
+   17.22%     0.00%             0  skyriver  [kernel.kallsyms]  [k] sys_read
+   16.77%     0.00%             1  skyriver  skyriver           [.] runtime.futex
+   16.29%     0.00%             0  skyriver  [kernel.kallsyms]  [k] sys_futex
+   16.28%     0.00%             1  skyriver  [kernel.kallsyms]  [k] do_futex
+   15.72%     0.56%             7  skyriver  [kernel.kallsyms]  [k] sock_read_iter
+   15.60%     0.00%             0  skyriver  [unknown]          [k] 0000000000000000
+   15.16%     0.00%             1  skyriver  [kernel.kallsyms]  [k] sock_recvmsg
+   14.93%     0.15%             3  skyriver  [kernel.kallsyms]  [k] inet_recvmsg
+   14.90%     2.10%            34  skyriver  skyriver           [.] runtime.runqsteal
+   14.13%     0.55%            10  skyriver  [kernel.kallsyms]  [k] tcp_recvmsg


+   63.56%     0.00%             0  skyriver  skyriver           [.] runtime.goexit
+   55.73%     0.05%             3  skyriver  [kernel.kallsyms]  [k] entry_SYSCALL_64_fastpath
+   47.89%     0.15%             1  skyriver  skyriver           [.] main.(*ConnectionHandler).serve
+   47.74%     0.00%             0  skyriver  skyriver           [.] main.(*ConnectionHandler).(main.receiveRequ
+   47.45%     0.00%             0  skyriver  skyriver           [.] main.(*ConnectionHandler).receiveRequestASC
+   46.64%     0.00%             0  skyriver  skyriver           [.] main.(*ConnectionHandler).receiveArgumentsA
+   46.41%     0.00%             0  skyriver  skyriver           [.] main.(*ConnectionHandler).receiveStorageFmt
+   45.98%     0.06%             1  skyriver  skyriver           [.] github.com/OmniTier/skyriver/slab/readBuffe
+   38.04%     0.01%             1  skyriver  skyriver           [.] net.(*conn).Read
+   37.75%     0.01%             2  skyriver  skyriver           [.] net.(*netFD).Read
+   37.66%     0.00%             0  skyriver  skyriver           [.] syscall.Syscall
+   35.78%     0.00%             0  skyriver  skyriver           [.] runtime.mcall
+   35.61%     0.00%             0  skyriver  skyriver           [.] runtime.park_m
+   35.18%     0.18%             2  skyriver  skyriver           [.] runtime.schedule
+   33.64%     0.00%             0  skyriver  [kernel.kallsyms]  [k] sys_read
+   33.53%     0.00%             0  skyriver  [kernel.kallsyms]  [k] vfs_read
+   33.09%     0.28%             1  skyriver  [kernel.kallsyms]  [k] new_sync_read
+   33.09%     0.00%             0  skyriver  [kernel.kallsyms]  [k] __vfs_read
+   32.21%     0.00%             1  skyriver  skyriver           [.] net.(*pollDesc).waitRead
+   32.01%     0.00%             0  skyriver  skyriver           [.] net.(*pollDesc).wait
+   32.01%     0.00%             0  skyriver  skyriver           [.] net.runtime_pollWait
+   31.76%     0.18%             1  skyriver  [kernel.kallsyms]  [k] sock_read_iter
+   31.57%     0.00%             0  skyriver  [kernel.kallsyms]  [k] sock_recvmsg
+   31.46%     0.37%             2  skyriver  skyriver           [.] runtime.netpollblock
+   31.13%     0.23%             2  skyriver  [kernel.kallsyms]  [k] inet_recvmsg
+   30.44%     1.12%             8  skyriver  [kernel.kallsyms]  [k] tcp_recvmsg
+   28.02%     1.65%            29  skyriver  skyriver           [.] runtime.findrunnable
+   21.76%     1.19%             9  skyriver  [kernel.kallsyms]  [k] skb_copy_datagram_iter
+   20.58%     0.04%             2  skyriver  [kernel.kallsyms]  [k] copy_page_to_iter
+   18.57%     0.00%             0  skyriver  [unknown]          [k] 0000000000000000
+   16.85%     0.00%             0  skyriver  skyriver           [.] main.workOnProtocol
+   16.56%     0.00%             0  skyriver  skyriver           [.] main.processProtocol
+   15.93%     5.27%            43  skyriver  skyriver           [.] runtime.memmove
+   15.58%     0.00%             1  skyriver  [kernel.kallsyms]  [k] __irqentry_text_start
+   15.58%     0.00%             0  skyriver  [kernel.kallsyms]  [k] ret_from_intr
+   15.43%     0.00%             1  skyriver  [kernel.kallsyms]  [k] __do_softirq
+   15.18%     0.00%             1  skyriver  [kernel.kallsyms]  [k] net_rx_action
+   15.03%     0.00%             0  skyriver  skyriver           [.] main.rmwWriteFill
+   15.01%     1.06%             9  skyriver  [kernel.kallsyms]  [k] ixgbe_poll
+   14.89%     0.00%             0  skyriver  [kernel.kallsyms]  [k] irq_exit
+   13.70%     0.00%             0  skyriver  skyriver           [.] main.(*ConnectionHandler).worker
+   12.65%    12.65%            92  skyriver  [kernel.kallsyms]  [k] copy_user_enhanced_fast_string
+   12.10%     1.95%            21  skyriver  skyriver           [.] runtime.runqsteal
+   10.00%     0.00%             1  skyriver  skyriver           [.] runtime.futex
+    9.98%     0.01%             2  skyriver  [kernel.kallsyms]  [k] sys_futex
+    9.97%     0.00%             1  skyriver  [kernel.kallsyms]  [k] do_futex
+    8.18%     0.41%             2  skyriver  [kernel.kallsyms]  [k] schedule
+    7.84%     0.25%             3  skyriver  skyriver           [.] runtime.epollwait
+    7.69%     1.52%            26  skyriver  [kernel.kallsyms]  [k] ixgbe_clean_rx_irq
+    7.67%     7.67%            46  skyriver  [unknown]          [k] 0x00000000004b3f20
+    7.43%     0.38%             4  skyriver  [kernel.kallsyms]  [k] __schedule
+    7.30%     5.38%            67  skyriver  skyriver           [.] runtime.runqgrab
+    7.24%     0.00%             1  skyriver  [kernel.kallsyms]  [k] sys_epoll_wait
+    7.12%     0.00%             1  skyriver  [kernel.kallsyms]  [k] napi_gro_complete
+    6.87%     0.00%             0  skyriver  [kernel.kallsyms]  [k] ep_poll
+    6.64%     0.00%             0  skyriver  [kernel.kallsyms]  [k] netif_receive_skb_internal
+    6.63%     0.00%             0  skyriver  [kernel.kallsyms]  [k] __netif_receive_skb
+    6.63%     0.01%             3  skyriver  [kernel.kallsyms]  [k] __netif_receive_skb_core
+    6.62%     0.01%             2  skyriver  [kernel.kallsyms]  [k] ip_rcv
+    6.55%     0.00%             0  skyriver  [kernel.kallsyms]  [k] __kfree_skb
+    6.23%     0.00%             0  skyriver  [kernel.kallsyms]  [k] skb_release_all
+    5.68%     0.00%             0  skyriver  [kernel.kallsyms]  [k] napi_gro_flush
+    5.42%     2.52%            17  skyriver  [kernel.kallsyms]  [k] put_page
+    5.36%     0.01%             2  skyriver  [kernel.kallsyms]  [k] futex_wait
+    5.17%     0.24%             2  skyriver  [kernel.kallsyms]  [k] ip_rcv_finish
+    4.92%     0.47%             4  skyriver  [kernel.kallsyms]  [k] skb_release_data
+    4.77%     0.00%             0  skyriver  [kernel.kallsyms]  [k] ip_local_deliver
+    4.60%     0.01%             2  skyriver  [kernel.kallsyms]  [k] futex_wait_queue_me
+    4.47%     0.00%             0  skyriver  [kernel.kallsyms]  [k] tcp_v4_do_rcv
+    4.47%     0.01%             2  skyriver  [kernel.kallsyms]  [k] tcp_rcv_established
+    4.41%     0.00%             1  skyriver  [kernel.kallsyms]  [k] futex_wake
+    4.31%     0.00%             0  skyriver  [kernel.kallsyms]  [k] ip_local_deliver_finish


$ go tool pprof skyriver profile.2s6m  
Entering interactive mode (type "help" for commands)
(pprof) top10
18590ms of 22020ms total (84.42%)
Dropped 99 nodes (cum <= 110.10ms)
Showing top 10 nodes out of 85 (cum >= 1580ms)
      flat  flat%   sum%        cum   cum%
    5650ms 25.66% 25.66%     6070ms 27.57%  syscall.Syscall
    4080ms 18.53% 44.19%     4090ms 18.57%  syscall.Syscall6
    2910ms 13.22% 57.40%     2910ms 13.22%  runtime.memmove
    1780ms  8.08% 65.49%     1780ms  8.08%  runtime.futex
    1560ms  7.08% 72.57%     1560ms  7.08%  runtime.epollwait
     930ms  4.22% 76.79%     1350ms  6.13%  runtime.runqgrab
     680ms  3.09% 79.88%     6850ms 31.11%  runtime.findrunnable
     410ms  1.86% 81.74%      410ms  1.86%  runtime/internal/atomic.Load
     360ms  1.63% 83.38%      360ms  1.63%  runtime/internal/atomic.Cas
     230ms  1.04% 84.42%     1580ms  7.18%  runtime.runqsteal


$ go tool pprof skyriver profile.2s32k 
Entering interactive mode (type "help" for commands)
(pprof) top10
45740ms of 54640ms total (83.71%)
Dropped 123 nodes (cum <= 273.20ms)
Showing top 10 nodes out of 73 (cum >= 660ms)
      flat  flat%   sum%        cum   cum%
   18660ms 34.15% 34.15%    19820ms 36.27%  syscall.Syscall
    6700ms 12.26% 46.41%     6700ms 12.26%  runtime.futex
    5490ms 10.05% 56.46%     5500ms 10.07%  syscall.Syscall6
    4830ms  8.84% 65.30%     4830ms  8.84%  runtime.memmove
    3470ms  6.35% 71.65%     3470ms  6.35%  runtime.epollwait
    2430ms  4.45% 76.10%     3290ms  6.02%  runtime.runqgrab
    1790ms  3.28% 79.37%    19700ms 36.05%  runtime.findrunnable
     900ms  1.65% 81.02%      900ms  1.65%  runtime/internal/atomic.Load
     810ms  1.48% 82.50%      810ms  1.48%  runtime/internal/atomic.Cas
     660ms  1.21% 83.71%      660ms  1.21%  runtime/internal/atomic.Xchg

sudo tcpdump -i p27p2 -w /tmp/2s32k.pcap

